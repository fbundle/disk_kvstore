below is an implementation of paxos algorithm
./dist_kvstore/kvstore.go
package dist_kvstore

import (
	"context"
	"github.com/dgraph-io/badger/v4"
	"github.com/khanh101/paxos/kvstore"
	"github.com/khanh101/paxos/paxos"
	"github.com/khanh101/paxos/rpc"
	"sync"
	"time"
)

type Store interface {
	Close() error
	RunLoop() error
	Get(key string) (string, bool)
	Set(key string, val string)
	Keys() []string
}

func makeHandlerFunc[Req any, Res any](acceptor paxos.Acceptor[command]) func(*Req) *Res {
	return func(req *Req) *Res {
		res := acceptor.Handle(req)
		if res == nil {
			return nil
		}
		return res.(*Res)
	}
}

type command struct {
	Key string `json:"key"`
	Val string `json:"val"`
}
type store struct {
	id           paxos.NodeId
	peerAddrList []string
	db           *badger.DB
	memStore     kvstore.MemStore[string, string]
	acceptor     paxos.Acceptor[command]
	server       rpc.TCPServer
	rpcList      []paxos.RPC
	writeMu      sync.Mutex
	updateCtx    context.Context
	updateCancel context.CancelFunc
}

func NewDistStore(id int, badgerPath string, peerAddrList []string) (Store, error) {
	bindAddr := peerAddrList[id]
	db, err := badger.Open(badger.DefaultOptions(badgerPath))
	if err != nil {
		return nil, err
	}
	acceptor := paxos.NewAcceptor[command](kvstore.NewBargerStore[paxos.LogId, paxos.Promise[command]](db))
	memStore := kvstore.NewMemStore[string, string]()
	acceptor.Listen(0, func(logId paxos.LogId, cmd command) {
		memStore.Update(func(txn kvstore.Txn[string, string]) any {
			if cmd.Val == "" {
				txn.Del(cmd.Key)
			} else {
				txn.Set(cmd.Key, cmd.Val)
			}
			return nil
		})
	})

	server, err := rpc.NewTCPServer(bindAddr)
	if err != nil {
		return nil, err
	}
	server = server.Append(
		"prepare", makeHandlerFunc[paxos.PrepareRequest, paxos.PrepareResponse](acceptor),
	).Append(
		"accept", makeHandlerFunc[paxos.AcceptRequest[command], paxos.AcceptResponse](acceptor),
	).Append(
		"commit", makeHandlerFunc[paxos.CommitRequest[command], paxos.CommitResponse](acceptor),
	).Append(
		"get", makeHandlerFunc[paxos.GetRequest, paxos.GetResponse[command]](acceptor),
	)

	rpcList := make([]paxos.RPC, len(peerAddrList))
	for i := range peerAddrList {
		if i == id {
			rpcList[i] = func(req paxos.Request, resCh chan<- paxos.Response) {
				resCh <- acceptor.Handle(req)
			}
		} else {
			rpcList[i] = func(req paxos.Request, resCh chan<- paxos.Response) {
				transport := rpc.TCPTransport(peerAddrList[i])
				res, err := func() (paxos.Response, error) {
					switch req.(type) {
					case *paxos.PrepareRequest:
						return rpc.RPC[paxos.PrepareRequest, paxos.PrepareResponse](transport, "prepare", req.(*paxos.PrepareRequest))
					case *paxos.AcceptRequest[command]:
						return rpc.RPC[paxos.AcceptRequest[command], paxos.AcceptResponse](transport, "accept", req.(*paxos.AcceptRequest[command]))
					case *paxos.CommitRequest[command]:
						return rpc.RPC[paxos.CommitRequest[command], paxos.CommitResponse](transport, "commit", req.(*paxos.CommitRequest[command]))
					case *paxos.GetRequest:
						return rpc.RPC[paxos.GetRequest, paxos.GetResponse[command]](transport, "get", req.(*paxos.GetRequest))
					default:
						return nil, nil
					}
				}()
				if err != nil {
					res = nil
				}
				resCh <- res
			}
		}
	}

	updateCtx, updateCancel := context.WithCancel(context.Background())
	return &store{
		id:           paxos.NodeId(id),
		peerAddrList: peerAddrList,
		db:           db,
		memStore:     memStore,
		acceptor:     acceptor,
		server:       server,
		rpcList:      rpcList,
		writeMu:      sync.Mutex{},
		updateCtx:    updateCtx,
		updateCancel: updateCancel,
	}, nil
}

func (ds *store) Close() error {
	ds.updateCancel()
	err1 := ds.db.Close()
	err2 := ds.server.Close()
	return combineErrors(err1, err2)
}

func (ds *store) RunLoop() error {
	go func() {
		for {
			select {
			case <-ds.updateCtx.Done():
				return
			default:
			}
			paxos.Update(ds.acceptor, ds.rpcList).Next()
			time.Sleep(100 * time.Millisecond) // update every 100 milliseconds
		}

	}()
	return ds.server.RunLoop()
}

func (ds *store) Get(key string) (string, bool) {
	o := ds.memStore.Update(func(txn kvstore.Txn[string, string]) any {
		val, ok := txn.Get(key)
		return [2]any{val, ok}
	}).([2]any)
	val, ok := o[0].(string), o[1].(bool)
	return val, ok
}

func (ds *store) Set(key string, val string) {
	ds.writeMu.Lock()
	defer ds.writeMu.Unlock()
	wait := time.Millisecond
	// exponential backoff
	backoff := func() {
		time.Sleep(wait)
		wait *= 2
	}
	for {
		logId := paxos.Update(ds.acceptor, ds.rpcList).Next()
		ok := paxos.Write(ds.acceptor, ds.id, logId, command{Key: key, Val: val}, ds.rpcList)
		if ok {
			break
		}
		backoff()
	}
}

func (ds *store) Keys() []string {
	return ds.memStore.Keys()
}
./dist_kvstore/util.go
package dist_kvstore

import "fmt"

func combineErrors(errs ...error) error {
	var errorList []error

	for _, err := range errs {
		if err == nil {
			continue
		}
		errorList = append(errorList, err)
	}
	if len(errorList) == 0 {
		return nil
	}
	return fmt.Errorf("%v", errorList)
}
./dist_kvstore/http.go
package dist_kvstore

import (
	"encoding/json"
	"io"
	"net/http"
	"strings"
)

func HttpHandle(ds Store) http.HandlerFunc {
	return func(w http.ResponseWriter, r *http.Request) {
		// expect path to be "/kvstore/<key>"
		key, ok := strings.CutPrefix(r.URL.Path, "/kvstore/")
		if !ok {
			http.Error(w, "invalid path, expected `/kvstore/<key>`", http.StatusBadRequest)
			return
		}
		if len(key) == 0 {
			keys := ds.Keys()
			b, err := json.Marshal(keys)
			if err != nil {
				http.Error(w, err.Error(), http.StatusInternalServerError)
			}
			_, err = w.Write(b)
			if err != nil {
				http.Error(w, err.Error(), http.StatusInternalServerError)
			}
			return
		}
		switch r.Method {
		case http.MethodGet:
			val, ok := ds.Get(key)
			if !ok {
				http.Error(w, "key not found", http.StatusNotFound)
				return
			}
			_, err := w.Write([]byte(val))
			if err != nil {
				http.Error(w, err.Error(), http.StatusInternalServerError)
				return
			}
		case http.MethodPost, http.MethodPut:
			body, err := io.ReadAll(r.Body)
			if err != nil {
				http.Error(w, err.Error(), http.StatusBadRequest)
				return
			}
			ds.Set(key, string(body))
			w.WriteHeader(http.StatusOK)
		case http.MethodDelete:
			ds.Set(key, "")
			w.WriteHeader(http.StatusOK)
		default:
			http.Error(w, "method must be GET POST PUT DELETE", http.StatusBadRequest)
			return
		}
	}
}
./paxos/simple_acceptor.go
package paxos

import (
	"github.com/khanh101/paxos/kvstore"
)

// ProposalNumber - roundId * 4294967296 + nodeId
type ProposalNumber uint64

const (
	ZERO     ProposalNumber = 0
	COMMITED ProposalNumber = 18446744073709551615
)

// Promise[T] - promise to reject all PREPARE if proposal <= this and all ACCEPT if proposal < this
type Promise[T any] struct {
	Proposal ProposalNumber `json:"proposal"`
	Value    T              `json:"value"`
}

func zero[T any]() T {
	var v T
	return v
}

type LogId uint64

type simpleAcceptor[T any] struct {
	log kvstore.Store[LogId, Promise[T]]
}

func getOrSetLogEntry[T any](txn kvstore.Txn[LogId, Promise[T]], logId LogId) (p Promise[T]) {
	if v, ok := txn.Get(logId); ok {
		return v
	}
	v := Promise[T]{
		Proposal: ZERO,
		Value:    zero[T](),
	}
	txn.Set(logId, v)
	return v
}

func (a *simpleAcceptor[T]) get(logId LogId) (promise Promise[T]) {
	return a.log.Update(func(txn kvstore.Txn[LogId, Promise[T]]) any {
		return getOrSetLogEntry(txn, logId)
	}).(Promise[T])
}

func (a *simpleAcceptor[T]) commit(logId LogId, v T) {
	a.log.Update(func(txn kvstore.Txn[LogId, Promise[T]]) any {
		txn.Set(logId, Promise[T]{
			Proposal: COMMITED,
			Value:    v,
		})
		return nil
	})
}

func (a *simpleAcceptor[T]) prepare(logId LogId, proposal ProposalNumber) (proposalOut ProposalNumber, ok bool) {
	r := a.log.Update(func(txn kvstore.Txn[LogId, Promise[T]]) any {
		p := getOrSetLogEntry(txn, logId)
		if p.Proposal == COMMITED { // reject if committed
			return [2]any{COMMITED, false}
		}
		if proposal <= p.Proposal { // fulfill promise
			return [2]any{p.Proposal, false}
		}
		// make new promise
		txn.Set(logId, Promise[T]{
			Proposal: proposal,
			Value:    p.Value, // old value
		})
		return [2]any{proposal, true}
	}).([2]any)
	return r[0].(ProposalNumber), r[1].(bool)
}

func (a *simpleAcceptor[T]) accept(logId LogId, proposal ProposalNumber, value T) (proposalOut ProposalNumber, ok bool) {
	r := a.log.Update(func(txn kvstore.Txn[LogId, Promise[T]]) any {
		p := getOrSetLogEntry(txn, logId)
		if p.Proposal == COMMITED { // reject if committed
			return [2]any{COMMITED, false}
		}
		if proposal < p.Proposal { // fulfill promise
			return [2]any{p.Proposal, false}
		}
		// make new promise
		txn.Set(logId, Promise[T]{
			Proposal: proposal,
			Value:    value, // new value
		})
		return [2]any{proposal, true}
	}).([2]any)
	return r[0].(ProposalNumber), r[1].(bool)
}
./paxos/acceptor.go
package paxos

import (
	"github.com/khanh101/paxos/kvstore"
	"sync"
)

type Acceptor[T any] interface {
	UpdateLocalCommit() Acceptor[T]
	Get(logId LogId) (val T, ok bool)
	Next() LogId
	Handle(req Request) (res Response)
	Listen(from LogId, listener func(logId LogId, value T)) (cancel func())
}

func NewAcceptor[T any](log kvstore.Store[LogId, Promise[T]]) Acceptor[T] {
	return (&acceptor[T]{
		mu: sync.Mutex{},
		acceptor: &simpleAcceptor[T]{
			log: log,
		},
		smallestUnapplied: 0,
		listenerCount:     0,
		listenerMap:       make(map[uint64]func(logId LogId, value T)),
	}).updateLocalCommitWithoutLock()
}

// acceptor - paxos acceptor must be persistent
type acceptor[T any] struct {
	mu                sync.Mutex
	acceptor          *simpleAcceptor[T]
	smallestUnapplied LogId
	listenerCount     uint64
	listenerMap       map[uint64]func(logId LogId, value T)
}

func (a *acceptor[T]) UpdateLocalCommit() Acceptor[T] {
	a.mu.Lock()
	defer a.mu.Unlock()
	return a.updateLocalCommitWithoutLock()
}

func (a *acceptor[T]) updateLocalCommitWithoutLock() *acceptor[T] {
	for {
		promise := a.acceptor.get(a.smallestUnapplied)
		if promise.Proposal != COMMITED {
			break
		}
		for _, listener := range a.listenerMap {
			listener(a.smallestUnapplied, promise.Value)
		}
		a.smallestUnapplied++
	}
	return a
}

func (a *acceptor[T]) Listen(from LogId, listener func(logId LogId, value T)) (cancel func()) {
	a.mu.Lock()
	defer a.mu.Unlock()
	if a.smallestUnapplied < from {
		panic("subscribe from a future log_id")
	}
	for logId := from; logId < a.smallestUnapplied; logId++ {
		listener(logId, a.acceptor.get(logId).Value)
	}
	count := a.listenerCount
	a.listenerCount++
	a.listenerMap[count] = listener
	return func() {
		a.mu.Lock()
		defer a.mu.Unlock()
		delete(a.listenerMap, count)
	}
}

func (a *acceptor[T]) Get(logId LogId) (T, bool) {
	a.mu.Lock()
	defer a.mu.Unlock()
	promise := a.acceptor.get(logId)
	return promise.Value, promise.Proposal == COMMITED
}

func (a *acceptor[T]) Next() LogId {
	a.mu.Lock()
	defer a.mu.Unlock()
	return a.smallestUnapplied
}
func (a *acceptor[T]) Handle(r Request) Response {
	a.mu.Lock()
	defer a.mu.Unlock()
	switch req := r.(type) {
	case *PrepareRequest:
		proposal, ok := a.acceptor.prepare(req.LogId, req.Proposal)
		return &PrepareResponse{
			Proposal: proposal,
			Ok:       ok,
		}
	case *AcceptRequest[T]:
		proposal, ok := a.acceptor.accept(req.LogId, req.Proposal, req.Value)
		return &AcceptResponse{
			Proposal: proposal,
			Ok:       ok,
		}
	case *CommitRequest[T]:
		a.acceptor.commit(req.LogId, req.Value)
		a.updateLocalCommitWithoutLock()
		return nil
	case *GetRequest:
		promise := a.acceptor.get(req.LogId)
		return &GetResponse[T]{
			Promise: promise,
		}
	default:
		return nil
	}
}
./paxos/message.go
package paxos

type Request interface {
}

type Response interface {
}
type PrepareRequest struct {
	LogId    LogId          `json:"log_id"`
	Proposal ProposalNumber `json:"proposal"`
}

type PrepareResponse struct {
	Proposal ProposalNumber `json:"proposal"`
	Ok       bool           `json:"ok"`
}

type AcceptRequest[T any] struct {
	LogId    LogId          `json:"log_id"`
	Proposal ProposalNumber `json:"proposal"`
	Value    T              `json:"value"`
}

type AcceptResponse struct {
	Proposal ProposalNumber `json:"proposal"`
	Ok       bool           `json:"ok"`
}

type CommitRequest[T any] struct {
	LogId LogId `json:"log_id"`
	Value T     `json:"value"`
}

type CommitResponse struct {
}

type GetRequest struct {
	LogId LogId `json:"log_id"`
}

type GetResponse[T any] struct {
	Promise Promise[T] `json:"promise"`
}
./paxos/proposer.go
package paxos

import "time"

type NodeId uint32

const (
	PROPOSAL_STEP ProposalNumber = 4294967296
)

func decompose(proposal ProposalNumber) (uint64, NodeId) {
	return uint64(proposal / PROPOSAL_STEP), NodeId(proposal % PROPOSAL_STEP)
}
func compose(round uint64, nodeId NodeId) ProposalNumber {
	return PROPOSAL_STEP*ProposalNumber(round) + ProposalNumber(nodeId)
}

func quorum(n int) int {
	return n/2 + 1
}

type RPC func(Request, chan<- Response)

func broadcast[Req any, Res any](rpcList []RPC, req Req) []Res {
	ch := make(chan Response, len(rpcList))
	defer close(ch)
	for _, rpc := range rpcList {
		rpc(req, ch)
	}
	resList := make([]Res, 0, len(rpcList))
	for range rpcList {
		res := <-ch
		if res == nil {
			continue
		}
		resList = append(resList, res.(Res))
	}
	return resList
}

// Update - check if there is an update
func Update[T any](a Acceptor[T], rpcList []RPC) Acceptor[T] {
	wait := time.Millisecond
	// exponential backoff
	backoff := func() {
		time.Sleep(wait)
		wait *= 2
	}
	for {
		logId := a.UpdateLocalCommit().Next()
		commited := false
		var v T
		for _, res := range broadcast[*GetRequest, *GetResponse[T]](rpcList, &GetRequest{
			LogId: logId,
		}) {
			if res.Promise.Proposal == COMMITED {
				v = res.Promise.Value
				commited = true
				break
			}
		}
		if !commited {
			break
		}
		a.Handle(&CommitRequest[T]{
			LogId: logId,
			Value: v,
		})
		backoff()
	}
	return a
}

// Write - write new value
func Write[T any](a Acceptor[T], id NodeId, logId LogId, value T, rpcList []RPC) bool {
	n := len(rpcList)
	proposal := compose(0, id)
	wait := time.Millisecond
	// exponential backoff
	backoff := func() {
		time.Sleep(wait)
		wait *= 2
	}
	for {
		if _, committed := Update(a, rpcList).Get(logId); committed {
			return false
		}
		// prepare
		{
			resList := broadcast[*PrepareRequest, *PrepareResponse](rpcList, &PrepareRequest{
				LogId:    logId,
				Proposal: proposal,
			})
			okCount := 0
			for _, res := range resList {
				if res.Ok {
					okCount++
				}
			}
			if okCount < quorum(n) {
				// update proposal
				maxRound := uint64(0)
				for _, res := range resList {
					round, _ := decompose(res.Proposal)
					if maxRound < round {
						maxRound = round
					}
				}
				proposal = compose(maxRound+1, id)
				backoff()
				continue
			}
		}
		// accept
		{
			resList := broadcast[*AcceptRequest[T], *AcceptResponse](rpcList, &AcceptRequest[T]{
				LogId:    logId,
				Proposal: proposal,
				Value:    value,
			})
			okCount := 0
			for _, res := range resList {
				if res.Ok {
					okCount++
				}
			}
			if okCount < quorum(n) {
				// update proposal
				maxRound := uint64(0)
				for _, res := range resList {
					round, _ := decompose(res.Proposal)
					if maxRound < round {
						maxRound = round
					}
				}
				proposal = compose(maxRound+1, id)
				backoff()
				continue
			}
		}
		// commit
		{
			// local commit
			a.Handle(&CommitRequest[T]{
				LogId: logId,
				Value: value,
			})
			// broadcast commit
			broadcast[*CommitRequest[T], *CommitResponse](rpcList, &CommitRequest[T]{
				LogId: logId,
				Value: value,
			})
		}
		return true
	}
}
./kvstore/barger.go
package kvstore

import (
	"encoding/json"
	"errors"
	"github.com/dgraph-io/badger/v4"
	"sync"
)

func NewBargerStore[K comparable, V any](db *badger.DB) Store[K, V] {

	return &badgerStore[K, V]{
		mu: sync.Mutex{},
		db: db,
	}
}

type badgerStore[K comparable, V any] struct {
	mu sync.Mutex
	db *badger.DB
}

type badgerTxn[K comparable, V any] struct {
	txn *badger.Txn
}

func (b *badgerTxn[K, V]) Get(k K) (v V, ok bool) {
	kb, err := json.Marshal(k)
	if err != nil {
		panic(err)
	}
	i, err := b.txn.Get(kb)
	if errors.Is(err, badger.ErrKeyNotFound) {
		return v, false
	}
	if err != nil {
		panic(err)
	}
	err = i.Value(func(val []byte) error {
		return json.Unmarshal(val, &v)
	})
	if err != nil {
		panic(err)
	}
	return v, true
}

func (b *badgerTxn[K, V]) Set(k K, v V) {
	kb, err := json.Marshal(k)
	if err != nil {
		panic(err)
	}
	vb, err := json.Marshal(v)
	if err != nil {
		panic(err)
	}
	err = b.txn.Set(kb, vb)
	if err != nil {
		panic(err)
	}
}

func (b *badgerTxn[K, V]) Del(k K) {
	kb, err := json.Marshal(k)
	if err != nil {
		panic(err)
	}
	err = b.txn.Delete(kb)
	if err != nil {
		panic(err)
	}
}

func (b *badgerStore[K, V]) Update(update func(txn Txn[K, V]) any) any {
	b.mu.Lock()
	defer b.mu.Unlock()
	var out any
	_ = b.db.Update(func(txn *badger.Txn) error {
		out = update(&badgerTxn[K, V]{txn: txn})
		return nil
	})
	return out
}

func (b *badgerStore[K, V]) Close() {
	err := b.db.Close()
	if err != nil {
		panic(err)
	}
}
./kvstore/store.go
package kvstore

import "sync"

// Store - supposed to be threadsafe for each key and persistent
type Store[K comparable, V any] interface {
	Update(update func(txn Txn[K, V]) any) any
}

type Txn[K comparable, V any] interface {
	Get(k K) (v V, ok bool)
	Set(k K, v V)
	Del(k K)
}

type MemStore[K comparable, V any] interface {
	Store[K, V]
	Keys() (keys []K)
}

func NewMemStore[K comparable, V any]() MemStore[K, V] {
	return &memStore[K, V]{
		mu:    sync.RWMutex{},
		store: make(map[K]V),
	}
}

type memStore[K comparable, V any] struct {
	mu    sync.RWMutex
	store map[K]V
}

func (m *memStore[K, V]) Keys() (keys []K) {
	m.mu.RLock()
	defer m.mu.RUnlock()
	keys = make([]K, 0, len(m.store))
	for k := range m.store {
		keys = append(keys, k)
	}
	return keys
}

func (m *memStore[K, V]) Update(update func(txn Txn[K, V]) any) any {
	m.mu.Lock()
	defer m.mu.Unlock()
	return update(m)
}

func (m *memStore[K, V]) Get(k K) (v V, ok bool) {
	v, ok = m.store[k]
	return v, ok
}

func (m *memStore[K, V]) Set(k K, v V) {
	m.store[k] = v
}

func (m *memStore[K, V]) Del(k K) {
	delete(m.store, k)
}
./rpc/dispatcher.go
package rpc

import (
	"encoding/json"
	"fmt"
	"reflect"
)

type handler struct {
	handlerFunc reflect.Value
	argType     reflect.Type
}

type Dispatcher struct {
	handlerMap map[string]handler
}

func NewDispatcher() *Dispatcher {
	return &Dispatcher{
		handlerMap: make(map[string]handler),
	}
}

type Message struct {
	Name string `json:"name"`
	Body []byte `json:"body"`
}

func (d *Dispatcher) Handle(input []byte) (output []byte, err error) {
	msg := Message{}
	if err := json.Unmarshal(input, &msg); err != nil {
		return nil, err
	}

	h, ok := d.handlerMap[msg.Name]
	if !ok {
		return nil, fmt.Errorf("command not found")
	}

	argPtr := reflect.New(h.argType.Elem()).Interface()
	if err := json.Unmarshal(msg.Body, argPtr); err != nil {
		return nil, err
	}

	out := h.handlerFunc.Call([]reflect.Value{reflect.ValueOf(argPtr)})[0].Interface()

	output, err = json.Marshal(out)
	if err != nil {
		return nil, err
	}
	return output, nil
}

func (d *Dispatcher) Append(name string, h any) *Dispatcher {
	hv := reflect.ValueOf(h)
	ht := hv.Type()
	if ht.Kind() != reflect.Func || ht.NumIn() != 1 || ht.NumOut() != 1 {
		panic("handler must be of form func(*SomeRequest) *SomeResponse")
	}
	arg := ht.In(0)
	if arg.Kind() != reflect.Ptr || ht.Out(0).Kind() != reflect.Ptr {
		panic("handler arguments and return type must be pointers")
	}
	d.handlerMap[name] = handler{
		handlerFunc: hv,
		argType:     arg,
	}
	return d
}
./rpc/tcp_rpc.go
package rpc

import (
	"bufio"
	"fmt"
	"io"
	"net"
	"sync"
)

type TCPServer interface {
	Handle(input []byte) (output []byte, err error)
	RunLoop() error
	Append(name string, h any) TCPServer
	Close() error
}

func TCPTransport(addr string) TransportFunc {
	return func(input []byte) (output []byte, err error) {
		conn, err := net.Dial("tcp", addr)
		if err != nil {
			return nil, err
		}
		defer conn.Close()
		conn.Write(input)
		conn.Write([]byte("\n")) // '\n' notifies end of input
		output, err = io.ReadAll(conn)
		return output, err
	}
}

type tcpServer struct {
	mu         sync.Mutex
	dispatcher *Dispatcher
	listener   net.Listener
}

func NewTCPServer(bindAddr string) (TCPServer, error) {
	listener, err := net.Listen("tcp", bindAddr)
	if err != nil {
		return nil, err
	}
	return &tcpServer{
		mu:         sync.Mutex{},
		dispatcher: NewDispatcher(),
		listener:   listener,
	}, nil
}

func (s *tcpServer) Close() error {
	return s.listener.Close()
}

func (s *tcpServer) Handle(input []byte) (output []byte, err error) {
	return s.dispatcher.Handle(input)
}

func (s *tcpServer) Append(name string, h any) TCPServer {
	s.mu.Lock()
	defer s.mu.Unlock()
	s.dispatcher.Append(name, h)
	return s
}

func (s *tcpServer) handleConn(conn net.Conn) {
	defer conn.Close()
	msg, err := bufio.NewReader(conn).ReadString('\n') // read until '\n'
	if err != nil {
		return
	}
	b := []byte(msg)
	{
		s.mu.Lock()
		defer s.mu.Unlock()
		b, err = s.dispatcher.Handle(b)
	}
	if err != nil {
		fmt.Println(err)
		return
	}
	conn.Write(b)
}

func (s *tcpServer) RunLoop() error {
	for {
		conn, err := s.listener.Accept()
		if err != nil {
			return err
		}
		go s.handleConn(conn)
	}
}
./rpc/rpc.go
package rpc

import (
	"encoding/json"
)

type TransportFunc func([]byte) ([]byte, error)

func zeroPtr[T any]() *T {
	var v T
	return &v
}

func RPC[Req any, Res any](transport TransportFunc, name string, req *Req) (res *Res, err error) {
	body, err := json.Marshal(req)
	if err != nil {
		return nil, err
	}
	msg := Message{
		Name: name,
		Body: body,
	}
	b, err := json.Marshal(msg)
	if err != nil {
		return nil, err
	}
	b, err = transport(b)
	if err != nil {
		return nil, err
	}

	res = zeroPtr[Res]()
	if err := json.Unmarshal(b, res); err != nil {
		return nil, err
	}
	return res, nil
}
./main.go
package main

import (
	"encoding/json"
	"fmt"
	"github.com/khanh101/paxos/dist_kvstore"
	"math/rand/v2"
	"net/http"
	"os"
	"strconv"
	"strings"
	"sync"
	"time"

	"github.com/khanh101/paxos/kvstore"
	"github.com/khanh101/paxos/paxos"
	"github.com/khanh101/paxos/rpc"
)

func testLocal() {
	n := 3

	// make 3 servers
	acceptorList := make([]paxos.Acceptor[string], n)
	for i := 0; i < n; i++ {
		i := i
		store := kvstore.NewMemStore[paxos.LogId, paxos.Promise[string]]()
		acceptorList[i] = paxos.NewAcceptor(store)
	}

	// TODO - make this tcp or http
	// define rpc communication -
	// drop 80% of requests and responses
	// in total, 0.96% of requests don't go through
	dropRate := 0.80
	rpcList := make([]paxos.RPC, n)
	for i := 0; i < n; i++ {
		i := i
		rpcList[i] = func(req paxos.Request, resCh chan<- paxos.Response) {
			go func() {
				if rand.Float64() < dropRate {
					resCh <- nil
					return
				}
				res := acceptorList[i].Handle(req)
				if rand.Float64() < dropRate {
					resCh <- nil
					return
				}
				resCh <- res
			}()
		}
	}

	listenerList := make([][]string, n)
	for i := 0; i < n; i++ {
		i := i
		acceptorList[i].Listen(0, func(logId paxos.LogId, value string) {
			fmt.Printf("acceptor %d log_id %d value %v\n", i, logId, value)
			listenerList[i] = append(listenerList[i], fmt.Sprintf("%v", value))
		})
	}

	// send updates at the same time
	wg := sync.WaitGroup{}
	for i := 0; i < n; i++ {
		wg.Add(1)
		go func(i int) {
			defer wg.Done()
			for j := 0; j < 5; j++ {
				v := fmt.Sprintf("value%d", i+3*j)
				for {
					// 1. update the acceptor
					// 2. get a new logId
					// 3. try to write the value to logId
					// 4. if failed, go back to 1
					logId := paxos.Update(acceptorList[i], rpcList).Next()
					ok := paxos.Write(acceptorList[i], paxos.NodeId(i), logId, v, rpcList)
					if ok {
						break
					}

					// time.Sleep(time.Duration(rand.Int()%100) * time.Millisecond)
				}
			}
		}(i)
	}

	wg.Wait()

	// update the servers
	dropRate = 0.0
	for i := 0; i < n; i++ {
		paxos.Update(acceptorList[i], rpcList)
	}
	// check the committed values
	// it should print the same 3 lines
	for i := 0; i < n; i++ {
		fmt.Println(strings.Join(listenerList[i], ""))
	}

	// new subscriber from 13
	for i := 0; i < n; i++ {
		acceptorList[i].Listen(13, func(logId paxos.LogId, value string) {
			fmt.Printf("%v", value)
		})
		fmt.Println()
	}

	return
}

func testRPC() {
	type AddReq struct {
		Values []int
	}

	type AddRes struct {
		Sum int
	}

	type SubReq struct {
		A int
		B int
	}

	type SubRes struct {
		Diff int
	}

	d := rpc.NewDispatcher()

	d.Append("add", func(req *AddReq) (res *AddRes) {
		sum := 0
		for _, v := range req.Values {
			sum += v
		}
		return &AddRes{
			Sum: sum,
		}
	}).Append("sub", func(req *SubReq) (res *SubRes) {
		return &SubRes{
			Diff: req.A - req.B,
		}
	})

	localTransport := d.Handle
	{
		res, err := rpc.RPC[AddReq, AddRes](
			localTransport,
			"add",
			&AddReq{Values: []int{1, 2, 3}},
		)
		if err != nil {
			panic(err)
		}
		fmt.Println(res)
	}
	{
		res, err := rpc.RPC[SubReq, SubRes](
			localTransport,
			"sub",
			&SubReq{A: 20, B: 16},
		)
		if err != nil {
			panic(err)
		}
		fmt.Println(res)
	}
}

func testRPCTCP() {
	type AddReq struct {
		Values []int
	}

	type AddRes struct {
		Sum int
	}

	type SubReq struct {
		A int
		B int
	}

	type SubRes struct {
		Diff int
	}

	addr := "localhost:14001"
	s, err := rpc.NewTCPServer(addr)
	if err != nil {
		panic(err)
	}
	defer s.Close()

	s.Append("add", func(req *AddReq) (res *AddRes) {
		sum := 0
		for _, v := range req.Values {
			sum += v
		}
		return &AddRes{
			Sum: sum,
		}
	}).Append("sub", func(req *SubReq) (res *SubRes) {
		return &SubRes{
			Diff: req.A - req.B,
		}
	})

	go s.RunLoop()

	transport := rpc.TCPTransport(addr)
	{
		res, err := rpc.RPC[AddReq, AddRes](
			transport,
			"add",
			&AddReq{Values: []int{1, 2, 3}},
		)
		if err != nil {
			panic(err)
		}
		fmt.Println(res)
	}
	{
		res, err := rpc.RPC[SubReq, SubRes](
			transport,
			"sub",
			&SubReq{A: 20, B: 16},
		)
		if err != nil {
			panic(err)
		}
		fmt.Println(res)
	}
}

func testDistKVStore() {
	badgerDBPathList := []string{
		"data/acceptor0",
		"data/acceptor1",
		"data/acceptor2",
	}
	peerAddrList := []string{
		"localhost:14000",
		"localhost:14001",
		"localhost:14002",
	}
	sList := make([]dist_kvstore.Store, 3)
	for i := 0; i < 3; i++ {
		s, err := dist_kvstore.NewDistStore(i, badgerDBPathList[i], peerAddrList)
		if err != nil {
			panic(err)
		}
		defer s.Close()
		go s.RunLoop()
		sList[i] = s
	}
	time.Sleep(time.Second)
	sList[0].Set("key1", "value1")
}

type Config struct {
	Badger string `json:"badger"`
	RPC    string `json:"rpc"`
	Store  string `json:"store"`
}
type ConfigList []Config

func main() {
	b, err := os.ReadFile(os.Args[1])
	if err != nil {
		panic(err)
	}
	var cl ConfigList
	err = json.Unmarshal(b, &cl)
	if err != nil {
		panic(err)
	}

	id, err := strconv.Atoi(os.Args[2])
	if err != nil {
		panic(err)
	}

	badgerDBPath := cl[id].Badger
	peerAddrList := make([]string, len(cl))
	for i, c := range cl {
		peerAddrList[i] = c.RPC
	}
	ds, err := dist_kvstore.NewDistStore(id, badgerDBPath, peerAddrList)
	if err != nil {
		panic(err)
	}
	defer ds.Close()
	go ds.RunLoop()
	time.Sleep(time.Second)

	// http server
	hs := &http.Server{
		Addr:    cl[id].Store,
		Handler: dist_kvstore.HttpHandle(ds),
	}
	fmt.Println("http server listening on", cl[id].Store)
	err = hs.ListenAndServe()
	if err != nil {
		panic(err)
	}
	return
}
./conf/kvstore.json
[
  {
    "badger": "data/acceptor0",
    "rpc": "localhost:3000",
    "store": "localhost:4000"
  },
  {
    "badger": "data/acceptor1",
    "rpc": "localhost:3001",
    "store": "localhost:4001"
  },
  {
    "badger": "data/acceptor2",
    "rpc": "localhost:3002",
    "store": "localhost:4002"
  }
]./README.md
# kvstore

implementation of distributed kvstore using paxos

paxos is easier to understand and prove. raft is unnecessarily complicated

[proof](https://github.com/khanh101/khanh101.github.io/blob/master/blog/pdf/paxos-algorithm.pdf)

[pkg.go.dev](https://pkg.go.dev/github.com/khanh101/paxos)

## EXAMPLE

cluster is online if and only if a quorum is online

```bash
go run main.go conf/kvstore.json 0
go run main.go conf/kvstore.json 1
go run main.go conf/kvstore.json 2
```

```bash
# get all keys
curl http://localhost:4000/kvstore/ -X GET
# write 
curl http://localhost:4000/kvstore/key1 -X PUT -d "value1"
# read
curl http://localhost:4000/kvstore/key1 -X GET
# delete 
curl http://localhost:4000/kvstore/key1 -X DELETE
```


do you have any comment on this?
