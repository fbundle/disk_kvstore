below is an implementation of paxos algorithm
./cmd/cat/main.go
package main

import (
	"fmt"
	"github.com/dgraph-io/badger/v4"
	"os"
	"strconv"
)

func main() {
	for i := 1; i < len(os.Args); i++ {
		badgerPath := os.Args[i]
		func(badgerPath string) {
			db, err := badger.Open(badger.DefaultOptions(badgerPath))
			if err != nil {
				panic(err)
			}
			defer db.Close()

			log := make(map[int]string)
			err = db.View(func(txn *badger.Txn) error {
				opts := badger.DefaultIteratorOptions
				it := txn.NewIterator(opts)
				defer it.Close()
				for it.Rewind(); it.Valid(); it.Next() {
					item := it.Item()
					keyBytes := item.Key()
					err := item.Value(func(valBytes []byte) error {
						key, err := strconv.Atoi(string(keyBytes))
						if err != nil {
							fmt.Println(err)
							return nil
						}
						val := string(valBytes)
						log[key] = val
						return nil
					})
					if err != nil {
						return err
					}
				}
				return nil
			})
			if err != nil {
				panic(err)
			}
			key := 0
			for {
				val, ok := log[key]
				if !ok {
					_, _ = fmt.Fprintf(os.Stderr, "stopped with %d keys left\n", len(log))
					break
				}
				fmt.Printf("%d: %s\n", key, val)
				delete(log, key)
				key++
			}
		}(badgerPath)
	}
}
./cmd/simple_paxos/simple_paxos.go
package simple_paxos

type Proposal uint64

type Value string

type Acceptor struct {
	Promise Proposal
	Value   *Value
}

func (a *Acceptor) Prepare(proposal Proposal) (Proposal, *Value, bool) {
	promise, acceptedValue := a.Promise, a.Value
	if !(promise < proposal) {
		return promise, acceptedValue, false
	}
	a.Promise = proposal
	return promise, acceptedValue, true
}

func (a *Acceptor) Accept(proposal Proposal, value Value) (Proposal, bool) {
	promise := a.Promise
	if !(promise <= proposal) {
		return promise, false
	}
	a.Promise = proposal
	a.Value = &value
	return promise, true
}

const (
	ProposalStep = 256
)

type ProposerId uint64
type Round uint64

func compose(round Round, id ProposerId) Proposal {
	return Proposal(uint64(round)*ProposalStep + uint64(id))
}

func decompose(proposal Proposal) (Round, ProposerId) {
	return Round(proposal / ProposalStep), ProposerId(proposal % ProposalStep)
}

func Propose(id ProposerId, acceptorList []*Acceptor, value Value) Value {
	quorum := len(acceptorList)/2 + 1
	round := Round(0)
	for {
		proposal := compose(round, id)
		// prepare phase
		maxPromise, maxValuePtr, ok := func() (Proposal, *Value, bool) {
			maxPromise := Proposal(0)
			maxValuePtr := (*Value)(nil)
			okCount := 0
			for _, a := range acceptorList {
				promise, valuePtr, ok := a.Prepare(proposal)
				if ok {
					okCount++
				}
				if maxPromise <= promise {
					maxPromise = promise
					maxValuePtr = valuePtr
				}
			}
			return maxPromise, maxValuePtr, okCount >= quorum
		}()
		if !ok {
			// backoff
			round, _ = decompose(maxPromise)
			round++
			continue
		}
		// accept phase
		if maxValuePtr == nil {
			maxValuePtr = &value
		}
		maxPromise, ok = func() (Proposal, bool) {
			maxPromise := Proposal(0)
			okCount := 0
			for _, a := range acceptorList {
				promise, ok := a.Accept(proposal, *maxValuePtr)
				if ok {
					okCount++
				}
				if maxPromise <= promise {
					maxPromise = promise
				}
			}
			return maxPromise, okCount >= quorum
		}()
		if !ok {
			// backoff
			round, _ = decompose(maxPromise)
			round++
			continue
		}
		// consensus has been reached at value (*maxValuePtr)
		return *maxValuePtr
	}

}
./cmd/test/main.go
package main

import (
	"fmt"
	"math/rand/v2"
	"strings"
	"sync"

	"github.com/khanh101/paxos/kvstore"
	"github.com/khanh101/paxos/paxos"
	"github.com/khanh101/paxos/rpc"
)

func testLocal() {
	n := 3

	// make 3 servers
	acceptorList := make([]paxos.Acceptor[string], n)
	for i := 0; i < n; i++ {
		i := i
		acceptorList[i] = paxos.NewAcceptor[string](kvstore.NewMemStore[paxos.LogId, paxos.Promise[string]]())
	}

	// TODO - make this tcp or http
	// define rpc communication -
	// drop 80% of requests and responses
	// in total, 0.96% of requests don't go through
	// disable waiting in paxos.Write to test this
	dropRate := 0.80
	rpcList := make([]paxos.RPC, n)
	for i := 0; i < n; i++ {
		i := i
		rpcList[i] = func(req paxos.Request, resCh chan<- paxos.Response) {
			go func() {
				if rand.Float64() < dropRate {
					resCh <- nil
					return
				}
				res := acceptorList[i].HandleRPC(req)
				if rand.Float64() < dropRate {
					resCh <- nil
					return
				}
				resCh <- res
			}()
		}
	}

	listenerList := make([][]string, n)
	for i := 0; i < n; i++ {
		i := i
		acceptorList[i].Subscribe(0, func(logId paxos.LogId, value string) {
			fmt.Printf("acceptor %d log_id %d value %v\n", i, logId, value)
			listenerList[i] = append(listenerList[i], fmt.Sprintf("%v", value))
		})
	}

	// send updates at the same time
	wg := sync.WaitGroup{}
	for i := 0; i < n; i++ {
		wg.Add(1)
		go func(i int) {
			defer wg.Done()
			for j := 0; j < 5; j++ {
				v := fmt.Sprintf("value%d", i+3*j)
				for {
					// 1. update the acceptor
					// 2. get a new logId
					// 3. try to write the value to logId
					// 4. if failed, go back to 1
					logId := acceptorList[i].Next()
					val, ok := paxos.Write(acceptorList[i], paxos.NodeId(i), logId, v, rpcList)
					if val == v && ok {
						break
					}
					paxos.Update(acceptorList[i], rpcList)
				}
			}
		}(i)
	}

	wg.Wait()

	// update the servers
	dropRate = 0.0
	for i := 0; i < n; i++ {
		paxos.Update(acceptorList[i], rpcList)
	}
	// check the committed values
	// it should print the same 3 lines
	for i := 0; i < n; i++ {
		fmt.Println(strings.Join(listenerList[i], ""))
	}

	return
}

func testRPC() {
	type AddReq struct {
		Values []int
	}

	type AddRes struct {
		Sum int
	}

	type SubReq struct {
		A int
		B int
	}

	type SubRes struct {
		Diff int
	}

	d := rpc.NewDispatcher()

	d.Register("add", func(req *AddReq) (res *AddRes) {
		sum := 0
		for _, v := range req.Values {
			sum += v
		}
		return &AddRes{
			Sum: sum,
		}
	}).Register("sub", func(req *SubReq) (res *SubRes) {
		return &SubRes{
			Diff: req.A - req.B,
		}
	})

	localTransport := d.Handle
	{
		res, err := rpc.RPC[AddReq, AddRes](
			localTransport,
			"add",
			&AddReq{Values: []int{1, 2, 3}},
		)
		if err != nil {
			panic(err)
		}
		fmt.Println(res)
	}
	{
		res, err := rpc.RPC[SubReq, SubRes](
			localTransport,
			"sub",
			&SubReq{A: 20, B: 16},
		)
		if err != nil {
			panic(err)
		}
		fmt.Println(res)
	}
}

func testRPCTCP() {
	type AddReq struct {
		Values []int
	}

	type AddRes struct {
		Sum int
	}

	type SubReq struct {
		A int
		B int
	}

	type SubRes struct {
		Diff int
	}

	addr := "localhost:14001"
	s, err := rpc.NewTCPServer(addr)
	if err != nil {
		panic(err)
	}
	defer s.Close()

	s.Register("add", func(req *AddReq) (res *AddRes) {
		sum := 0
		for _, v := range req.Values {
			sum += v
		}
		return &AddRes{
			Sum: sum,
		}
	}).Register("sub", func(req *SubReq) (res *SubRes) {
		return &SubRes{
			Diff: req.A - req.B,
		}
	})

	go s.ListenAndServe()

	transport := rpc.TCPTransport(addr)
	{
		res, err := rpc.RPC[AddReq, AddRes](
			transport,
			"add",
			&AddReq{Values: []int{1, 2, 3}},
		)
		if err != nil {
			panic(err)
		}
		fmt.Println(res)
	}
	{
		res, err := rpc.RPC[SubReq, SubRes](
			transport,
			"sub",
			&SubReq{A: 20, B: 16},
		)
		if err != nil {
			panic(err)
		}
		fmt.Println(res)
	}
}

func main() {
	testLocal()
}
./dist_kvstore/kvstore.go
package dist_kvstore

import (
	"context"
	"math/rand"
	"sync"
	"time"

	"github.com/dgraph-io/badger/v4"
	"github.com/khanh101/paxos/kvstore"
	"github.com/khanh101/paxos/paxos"
	"github.com/khanh101/paxos/rpc"
)

const (
	BACKOFF_MIN_TIME = 10 * time.Millisecond
	BACKOFF_MAX_TIME = 1000 * time.Millisecond
	UPDATE_INTERVAL  = 100 * time.Millisecond
)

type Store interface {
	Close() error
	ListenAndServeRPC() error
	Get(key string) Entry
	Set(Cmd)
	Keys() []string
}

func makeHandlerFunc[Req any, Res any](acceptor paxos.Acceptor[Cmd]) func(*Req) *Res {
	return func(req *Req) *Res {
		res := acceptor.HandleRPC(req)
		if res == nil {
			return nil
		}
		return res.(*Res)
	}
}

type store struct {
	id           paxos.NodeId
	peerAddrList []string
	db           *badger.DB
	memStore     *stateMachine
	acceptor     paxos.Acceptor[Cmd]
	server       rpc.TCPServer
	rpcList      []paxos.RPC
	writeMu      sync.Mutex
	updateCtx    context.Context
	updateCancel context.CancelFunc
}

func getDefaultEntry(txn kvstore.Txn[string, Entry], key string) Entry {
	entry, ok := txn.Get(key)
	if !ok {
		return Entry{
			Key: key,
			Val: "",
			Ver: 0,
		}
	}
	return entry
}

func NewDistStore(id int, badgerPath string, peerAddrList []string) (Store, error) {
	bindAddr := peerAddrList[id]
	db, err := badger.Open(badger.DefaultOptions(badgerPath))
	if err != nil {
		return nil, err
	}
	acceptor := paxos.NewAcceptor(kvstore.NewBargerStore[paxos.LogId, paxos.Promise[Cmd]](db))
	memStore := newStateMachine()
	acceptor.Subscribe(0, memStore.Apply)

	server, err := rpc.NewTCPServer(bindAddr)
	if err != nil {
		return nil, err
	}
	server = server.
		Register("prepare", makeHandlerFunc[paxos.PrepareRequest, paxos.PrepareResponse[Cmd]](acceptor)).
		Register("accept", makeHandlerFunc[paxos.AcceptRequest[Cmd], paxos.AcceptResponse[Cmd]](acceptor)).
		Register("commit", makeHandlerFunc[paxos.CommitRequest[Cmd], paxos.CommitResponse](acceptor)).
		Register("poll", makeHandlerFunc[paxos.PollRequest, paxos.PollResponse[Cmd]](acceptor))

	rpcList := make([]paxos.RPC, len(peerAddrList))
	for i := range peerAddrList {
		i := i
		if i == id {
			rpcList[i] = func(req paxos.Request, resCh chan<- paxos.Response) {
				resCh <- acceptor.HandleRPC(req)
			}
		} else {
			rpcList[i] = func(req paxos.Request, resCh chan<- paxos.Response) {
				transport := rpc.TCPTransport(peerAddrList[i])
				res, err := func() (paxos.Response, error) {
					switch req.(type) {
					case *paxos.PrepareRequest:
						return rpc.RPC[paxos.PrepareRequest, paxos.PrepareResponse[Cmd]](transport, "prepare", req.(*paxos.PrepareRequest))
					case *paxos.AcceptRequest[Cmd]:
						return rpc.RPC[paxos.AcceptRequest[Cmd], paxos.AcceptResponse[Cmd]](transport, "accept", req.(*paxos.AcceptRequest[Cmd]))
					case *paxos.CommitRequest[Cmd]:
						return rpc.RPC[paxos.CommitRequest[Cmd], paxos.CommitResponse](transport, "commit", req.(*paxos.CommitRequest[Cmd]))
					case *paxos.PollRequest:
						return rpc.RPC[paxos.PollRequest, paxos.PollResponse[Cmd]](transport, "poll", req.(*paxos.PollRequest))
					default:
						return nil, nil
					}
				}()
				if err != nil {
					res = nil
				}
				resCh <- res
			}
		}
	}

	updateCtx, updateCancel := context.WithCancel(context.Background())
	return &store{
		id:           paxos.NodeId(id),
		peerAddrList: peerAddrList,
		db:           db,
		memStore:     memStore,
		acceptor:     acceptor,
		server:       server,
		rpcList:      rpcList,
		writeMu:      sync.Mutex{},
		updateCtx:    updateCtx,
		updateCancel: updateCancel,
	}, nil
}

func (ds *store) Close() error {
	ds.updateCancel()
	err1 := ds.db.Close()
	err2 := ds.server.Close()
	return combineErrors(err1, err2)
}

func (ds *store) ListenAndServeRPC() error {
	go func() {
		ticker := time.NewTicker(UPDATE_INTERVAL)
		defer ticker.Stop()
		for {
			select {
			case <-ds.updateCtx.Done():
				return
			case <-ticker.C:
				paxos.Update(ds.acceptor, ds.rpcList)
			}
		}

	}()
	return ds.server.ListenAndServe()
}

func (ds *store) Set(cmd Cmd) {
	ds.writeMu.Lock()
	defer ds.writeMu.Unlock()
	wait := BACKOFF_MIN_TIME
	backoff := func() {
		time.Sleep(time.Duration(rand.Intn(int(wait))))
		wait *= 2
		if wait > BACKOFF_MAX_TIME {
			wait = BACKOFF_MAX_TIME
		}
	}
	for {
		logId := ds.acceptor.Next()
		value, ok := paxos.Write(ds.acceptor, ds.id, logId, cmd, ds.rpcList)
		if ok && value.Equal(cmd) {
			break
		}
		backoff()
	}
}

func (ds *store) Get(key string) Entry {
	return ds.memStore.Get(key)
}

func (ds *store) Keys() []string {
	return ds.memStore.Keys()
}
./dist_kvstore/state_machine.go
package dist_kvstore

import (
	"github.com/google/uuid"
	"github.com/khanh101/paxos/kvstore"
	"github.com/khanh101/paxos/paxos"
)

type Entry struct {
	Key string `json:"key"`
	Val string `json:"val"`
	Ver uint64 `json:"ver"`
}

type Cmd struct {
	Uuid    uuid.UUID `json:"uuid"`
	Entries []Entry   `json:"entries"`
}

func makeCmd(entries []Entry) Cmd {
	return Cmd{
		Uuid:    uuid.New(),
		Entries: entries,
	}
}

func (cmd Cmd) Equal(other Cmd) bool {
	return cmd.Uuid == other.Uuid
}

type stateMachine struct {
	store kvstore.MemStore[string, Entry]
}

func newStateMachine() *stateMachine {
	return &stateMachine{
		store: kvstore.NewMemStore[string, Entry](),
	}
}

func (sm *stateMachine) Get(key string) Entry {
	return sm.store.Update(func(txn kvstore.Txn[string, Entry]) any {
		return getDefaultEntry(txn, key)
	}).(Entry)
}

func (sm *stateMachine) Keys() []string {
	return sm.store.Update(func(txn kvstore.Txn[string, Entry]) any {
		return sm.store.Keys()
	}).([]string)
}

func (sm *stateMachine) Apply(logId paxos.LogId, cmd Cmd) {
	sm.store.Update(func(txn kvstore.Txn[string, Entry]) any {
		for _, entry := range cmd.Entries {
			oldEntry := getDefaultEntry(txn, entry.Key)
			if entry.Ver <= oldEntry.Ver {
				continue // ignore update
			}
			if len(entry.Val) == 0 {
				txn.Del(entry.Key)
			} else {
				txn.Set(entry.Key, entry)
			}
		}
		return nil
	})
}
./dist_kvstore/util.go
package dist_kvstore

import "fmt"

func combineErrors(errs ...error) error {
	var errorList []error

	for _, err := range errs {
		if err == nil {
			continue
		}
		errorList = append(errorList, err)
	}
	if len(errorList) == 0 {
		return nil
	}
	return fmt.Errorf("%v", errorList)
}
./dist_kvstore/http.go
package dist_kvstore

import (
	"encoding/json"
	"io"
	"net/http"
	"strings"
)

type versionedValue struct {
	Val string `json:"val"`
	Ver uint64 `json:"ver"`
}

func HttpHandle(ds Store) http.HandlerFunc {
	return func(w http.ResponseWriter, r *http.Request) {
		if !strings.HasPrefix(r.URL.Path, "/kvstore/") {
			http.NotFound(w, r)
			return
		}
		defer r.Body.Close()

		key, _ := strings.CutPrefix(r.URL.Path, "/kvstore/")
		if len(key) == 0 {
			b, err := json.Marshal(ds.Keys())
			if err != nil {
				http.Error(w, err.Error(), http.StatusInternalServerError)
			}
			_, err = w.Write(b)
			if err != nil {
				http.Error(w, err.Error(), http.StatusInternalServerError)
			}
			return
		}
		switch r.Method {
		case http.MethodGet:
			entry := ds.Get(key)
			b, err := json.Marshal(entry)
			if err != nil {
				http.Error(w, err.Error(), http.StatusInternalServerError)
				return
			}
			_, err = w.Write(b)
			if err != nil {
				http.Error(w, err.Error(), http.StatusInternalServerError)
				return
			}
		case http.MethodPost, http.MethodPut:
			body, err := io.ReadAll(r.Body)
			if err != nil {
				http.Error(w, err.Error(), http.StatusBadRequest)
				return
			}
			v := versionedValue{}
			err = json.Unmarshal(body, &v)
			if err != nil {
				http.Error(w, err.Error(), http.StatusBadRequest)
				return
			}
			cmd := makeCmd([]Entry{
				{
					Key: key,
					Val: v.Val,
					Ver: v.Ver,
				},
			})

			ds.Set(cmd)
			w.WriteHeader(http.StatusOK)
		default:
			http.Error(w, "method must be GET POST PUT", http.StatusBadRequest)
		}

	}
}
./paxos/simple_acceptor.go
package paxos

import (
	"github.com/khanh101/paxos/kvstore"
)

// ProposalNumber - roundId * 4294967296 + nodeId
type ProposalNumber uint64

type LogId uint64

const (
	INITIAL   ProposalNumber = 0
	COMMITTED ProposalNumber = 18446744073709551615
)

// Promise - promise to reject all PREPARE if proposal <= this and all ACCEPT if proposal < this
type Promise[T any] struct {
	Proposal ProposalNumber `json:"proposal"`
	Value    *T             `json:"value"`
}

func zero[T any]() T {
	var v T
	return v
}

type simpleAcceptor[T any] struct {
	log kvstore.Store[LogId, Promise[T]]
}

func getDefaultLogEntry[T any](txn kvstore.Txn[LogId, Promise[T]], logId LogId) (p Promise[T]) {
	v, ok := txn.Get(logId)
	if !ok {
		return Promise[T]{
			Proposal: INITIAL,
			Value:    nil,
		}
	}
	return v
}

func (a *simpleAcceptor[T]) get(logId LogId) (ProposalNumber, *T) {
	promise := a.log.Update(func(txn kvstore.Txn[LogId, Promise[T]]) any {
		return getDefaultLogEntry(txn, logId)
	}).(Promise[T])
	return promise.Proposal, promise.Value
}

func (a *simpleAcceptor[T]) commit(logId LogId, v T) {
	a.log.Update(func(txn kvstore.Txn[LogId, Promise[T]]) any {
		txn.Set(logId, Promise[T]{
			Proposal: COMMITTED,
			Value:    &v,
		})
		return nil
	})
}

func (a *simpleAcceptor[T]) prepare(logId LogId, proposal ProposalNumber) (Promise[T], bool) {
	r := a.log.Update(func(txn kvstore.Txn[LogId, Promise[T]]) any {
		p := getDefaultLogEntry(txn, logId)
		if !(p.Proposal < proposal) {
			return [2]any{p, false}
		}
		txn.Set(logId, Promise[T]{
			Proposal: proposal,
			Value:    p.Value,
		})
		return [2]any{p, true}
	}).([2]any)
	promise, ok := r[0].(Promise[T]), r[1].(bool)
	return promise, ok
}

func (a *simpleAcceptor[T]) accept(logId LogId, proposal ProposalNumber, value T) (Promise[T], bool) {
	r := a.log.Update(func(txn kvstore.Txn[LogId, Promise[T]]) any {
		p := getDefaultLogEntry(txn, logId)
		if !(p.Proposal <= proposal) {
			return [2]any{p, false}
		}
		txn.Set(logId, Promise[T]{
			Proposal: proposal,
			Value:    &value,
		})
		return [2]any{p, true}
	}).([2]any)
	promise, ok := r[0].(Promise[T]), r[1].(bool)
	return promise, ok
}
./paxos/acceptor.go
package paxos

import (
	"sync"

	"github.com/khanh101/paxos/kvstore"
)

type StateMachine[T any] func(logId LogId, value T)

type Acceptor[T any] interface {
	// GetValue - get value
	GetValue(logId LogId) (val T, ok bool)
	// Next - get smallestUnapplied - used to propose
	Next() LogId
	// HandleRPC - handle RPC requests
	HandleRPC(req Request) (res Response)
	// Subscribe - subscribe a state machine to log
	// smallestUnapplied is the index when state machine will start getting updates
	// it ignores all previous log entries
	Subscribe(smallestUnapplied LogId, sm StateMachine[T]) (cancel func())
}

func NewAcceptor[T any](log kvstore.Store[LogId, Promise[T]]) Acceptor[T] {
	return (&acceptor[T]{
		mu:                sync.Mutex{},
		acceptor:          &simpleAcceptor[T]{log: log},
		smallestUnapplied: 0,
		subsciber:         nil,
	}).applyCommitWithoutLock()
}

// acceptor - paxos acceptor must be persistent
type acceptor[T any] struct {
	mu                sync.Mutex
	acceptor          *simpleAcceptor[T]
	smallestUnapplied LogId
	subsciber         StateMachine[T]
}

func (a *acceptor[T]) applyCommitWithoutLock() *acceptor[T] {
	for {
		proposal, value := a.acceptor.get(a.smallestUnapplied)
		if proposal != COMMITTED {
			break
		}
		if a.subsciber != nil {
			a.subsciber(a.smallestUnapplied, *value)
		}
		a.smallestUnapplied++
	}
	return a
}

func (a *acceptor[T]) Subscribe(smallestUnapplied LogId, sm StateMachine[T]) (cancel func()) {
	a.mu.Lock()
	defer a.mu.Unlock()
	if a.subsciber != nil {
		panic("cannot subscribe twice")
	}
	a.subsciber = sm
	a.smallestUnapplied = smallestUnapplied

	return func() {
		a.mu.Lock()
		defer a.mu.Unlock()
		a.subsciber = nil
	}
}

func (a *acceptor[T]) GetValue(logId LogId) (T, bool) {
	a.mu.Lock()
	defer a.mu.Unlock()
	proposal, value := a.acceptor.get(logId)
	if proposal == COMMITTED {
		return *value, true
	}
	return zero[T](), false
}

func (a *acceptor[T]) Next() LogId {
	a.mu.Lock()
	defer a.mu.Unlock()
	return a.applyCommitWithoutLock().smallestUnapplied
}

func (a *acceptor[T]) HandleRPC(r Request) Response {
	a.mu.Lock()
	defer a.mu.Unlock()
	switch req := r.(type) {
	case *PrepareRequest:
		promise, ok := a.acceptor.prepare(req.LogId, req.Proposal)
		return &PrepareResponse[T]{
			Promise: promise,
			Ok:      ok,
		}
	case *AcceptRequest[T]:
		promise, ok := a.acceptor.accept(req.LogId, req.Proposal, req.Value)
		return &AcceptResponse[T]{
			Promise: promise,
			Ok:      ok,
		}
	case *CommitRequest[T]:
		a.acceptor.commit(req.LogId, req.Value)
		a.applyCommitWithoutLock()
		return nil
	case *PollRequest:
		proposal, value := a.acceptor.get(req.LogId)
		return &PollResponse[T]{
			Proposal: proposal,
			Value:    value,
		}
	default:
		return nil
	}
}
./paxos/message.go
package paxos

type Request interface {
}

type Response interface {
}
type PrepareRequest struct {
	LogId    LogId          `json:"log_id"`
	Proposal ProposalNumber `json:"proposal"`
}

type PrepareResponse[T any] struct {
	Promise Promise[T] `json:"promise"`
	Ok      bool       `json:"ok"`
}

type AcceptRequest[T any] struct {
	LogId    LogId          `json:"log_id"`
	Proposal ProposalNumber `json:"proposal"`
	Value    T              `json:"value"`
}

type AcceptResponse[T any] struct {
	Promise Promise[T] `json:"promise"`
	Ok      bool       `json:"ok"`
}

type CommitRequest[T any] struct {
	LogId LogId `json:"log_id"`
	Value T     `json:"value"`
}

type CommitResponse struct {
}

type PollRequest struct {
	LogId LogId `json:"log_id"`
}

type PollResponse[T any] struct {
	Proposal ProposalNumber `json:"proposal"`
	Value    *T             `json:"value"`
}
./paxos/proposer.go
package paxos

import (
	"math/rand"
	"time"
)

type NodeId uint64

const (
	PROPOSAL_STEP    ProposalNumber = 4294967296
	BACKOFF_MIN_TIME time.Duration  = 10 * time.Millisecond
	BACKOFF_MAX_TIME time.Duration  = 1000 * time.Millisecond
)

func decompose(proposal ProposalNumber) (uint64, NodeId) {
	return uint64(proposal / PROPOSAL_STEP), NodeId(proposal % PROPOSAL_STEP)
}
func compose(round uint64, nodeId NodeId) ProposalNumber {
	return PROPOSAL_STEP*ProposalNumber(round) + ProposalNumber(nodeId)
}

func quorum(n int) int {
	return n/2 + 1
}

type RPC func(Request, chan<- Response)

func broadcast[Req any, Res any](rpcList []RPC, req Req) []Res {
	ch := make(chan Response, len(rpcList))
	defer close(ch)
	for _, rpc := range rpcList {
		rpc(req, ch)
	}
	resList := make([]Res, 0, len(rpcList))
	for range rpcList {
		res := <-ch
		if res == nil {
			continue
		}
		resList = append(resList, res.(Res))
	}
	return resList
}

// Update - check if there is an update
func Update[T any](a Acceptor[T], rpcList []RPC) Acceptor[T] {
	for {
		logId := a.Next()
		commited := false
		var v T
		for _, res := range broadcast[*PollRequest, *PollResponse[T]](rpcList, &PollRequest{
			LogId: logId,
		}) {
			if res.Proposal == COMMITTED {
				v = *res.Value
				commited = true
				break
			}
		}
		if !commited {
			break
		}
		a.HandleRPC(&CommitRequest[T]{
			LogId: logId,
			Value: v,
		})
	}
	return a
}

// Write - write new value
func Write[T any](a Acceptor[T], id NodeId, logId LogId, value T, rpcList []RPC) (T, bool) {
	n := len(rpcList)
	proposal := compose(0, id)
	wait := BACKOFF_MIN_TIME
	// exponential backoff
	backoff := func() {
		a = Update(a, rpcList)
		time.Sleep(time.Duration(rand.Intn(int(wait))))
		wait *= 2
		if wait > BACKOFF_MAX_TIME {
			wait = BACKOFF_MAX_TIME
		}
	}
	for {
		if _, committed := a.GetValue(logId); committed {
			return zero[T](), false
		}
		var highestValue T                 // non-nil value assigned with the highest proposal number
		var highestProposal ProposalNumber // highest promised proposal number on all acceptors - used to choose the next proposal number
		var okCount int                    // number of acceptors promised to this request
		// prepare
		highestValue, highestProposal, okCount = func() (T, ProposalNumber, int) {
			resList := broadcast[*PrepareRequest, *PrepareResponse[T]](rpcList, &PrepareRequest{
				LogId:    logId,
				Proposal: proposal,
			})

			maxValuePtr := (*T)(nil)
			maxProposal := ProposalNumber(0)
			okCount := 0
			for _, res := range resList {
				if res.Ok {
					okCount++
				}
				if maxProposal <= res.Promise.Proposal {
					// propagate the value with the highest proposal number
					// this is actually not important for consensus
					// but to prevent proposers sending too many different values
					// this is a mechanism to promote convergence
					maxProposal = res.Promise.Proposal
					maxValuePtr = res.Promise.Value
				}
			}
			if maxValuePtr == nil {
				maxValuePtr = &value
			}
			return *maxValuePtr, maxProposal, okCount
		}()

		if okCount < quorum(n) {
			maxRound, _ := decompose(highestProposal)
			proposal = compose(maxRound+1, id)
			backoff()
			continue
		}
		// accept
		highestProposal, okCount = func() (ProposalNumber, int) {
			resList := broadcast[*AcceptRequest[T], *AcceptResponse[T]](rpcList, &AcceptRequest[T]{
				LogId:    logId,
				Proposal: proposal,
				Value:    highestValue,
			})
			maxProposal := ProposalNumber(0)
			okCount := 0
			for _, res := range resList {
				if res.Ok {
					okCount++
				}
				if maxProposal < res.Promise.Proposal {
					maxProposal = res.Promise.Proposal
				}
			}
			return maxProposal, okCount
		}()
		if okCount < quorum(n) {
			maxRound, _ := decompose(highestProposal)
			proposal = compose(maxRound+1, id)
			backoff()
			continue
		}
		// commit
		func() {
			// broadcast commit
			go broadcast[*CommitRequest[T], *CommitResponse](rpcList, &CommitRequest[T]{
				LogId: logId,
				Value: highestValue,
			})
			// local commit
			a.HandleRPC(&CommitRequest[T]{
				LogId: logId,
				Value: highestValue,
			})
		}()
		return highestValue, true
	}
}

func LogCompact[T any](rpcList []RPC) {
	// TODO
	// 1. send request to get smallest logId
	// 2. emit log compaction request
	// 3. T is a sum type of Command CompressedCommand.
	//    compact log entries [0, 1, 2, 3, ...] -> [x, 2, 3, ...]
	//    where x stores CompressedCommand
	// 4. StateMachine applies CompressedCommand is recovering from snapshot
	// 5. initLogId is used to restore the StateMachine

	/*

	 */
	// compressLog[T any](ts ...T) T // type signature for log compression where T is type of value
}
./kvstore/mem_store.go
package kvstore

import "sync"

func NewMemStore[K comparable, V any]() MemStore[K, V] {
	return &memStore[K, V]{
		mu:    sync.Mutex{},
		store: make(map[K]V),
	}
}

type memStore[K comparable, V any] struct {
	mu    sync.Mutex
	store map[K]V
}

func (m *memStore[K, V]) Update(update func(txn Txn[K, V]) any) any {
	m.mu.Lock()
	defer m.mu.Unlock()
	return update(m)
}

func (m *memStore[K, V]) Keys() (keys []K) {
	keys = make([]K, 0, len(m.store))
	for k := range m.store {
		keys = append(keys, k)
	}
	return keys
}

func (m *memStore[K, V]) Get(k K) (v V, ok bool) {
	v, ok = m.store[k]
	return v, ok
}

func (m *memStore[K, V]) Set(k K, v V) {
	m.store[k] = v
}

func (m *memStore[K, V]) Del(k K) {
	delete(m.store, k)
}
./kvstore/store.go
package kvstore

type Txn[K comparable, V any] interface {
	Get(k K) (v V, ok bool)
	Set(k K, v V)
	Del(k K)
}

// Store - threadsafe stable store
type Store[K comparable, V any] interface {
	Update(update func(txn Txn[K, V]) any) any
}

type MemStore[K comparable, V any] interface {
	Store[K, V]
	Keys() []K
}
./kvstore/barger_store.go
package kvstore

import (
	"encoding/json"
	"errors"
	"github.com/dgraph-io/badger/v4"
	"sync"
)

func NewBargerStore[K comparable, V any](db *badger.DB) Store[K, V] {
	return &badgerStore[K, V]{
		mu: sync.Mutex{},
		db: db,
	}
}

type badgerStore[K comparable, V any] struct {
	mu sync.Mutex
	db *badger.DB
}

type badgerTxn[K comparable, V any] struct {
	txn *badger.Txn
}

func (b *badgerTxn[K, V]) Get(k K) (v V, ok bool) {
	kb, err := json.Marshal(k)
	if err != nil {
		panic(err)
	}
	i, err := b.txn.Get(kb)
	if errors.Is(err, badger.ErrKeyNotFound) {
		return v, false
	}
	if err != nil {
		panic(err)
	}
	err = i.Value(func(val []byte) error {
		return json.Unmarshal(val, &v)
	})
	if err != nil {
		panic(err)
	}
	return v, true
}

func (b *badgerTxn[K, V]) Set(k K, v V) {
	kb, err := json.Marshal(k)
	if err != nil {
		panic(err)
	}
	vb, err := json.Marshal(v)
	if err != nil {
		panic(err)
	}
	err = b.txn.Set(kb, vb)
	if err != nil {
		panic(err)
	}
}

func (b *badgerTxn[K, V]) Del(k K) {
	kb, err := json.Marshal(k)
	if err != nil {
		panic(err)
	}
	err = b.txn.Delete(kb)
	if err != nil {
		panic(err)
	}
}

func (b *badgerStore[K, V]) Update(update func(txn Txn[K, V]) any) any {
	b.mu.Lock()
	defer b.mu.Unlock()
	var out any
	_ = b.db.Update(func(txn *badger.Txn) error {
		out = update(&badgerTxn[K, V]{txn: txn})
		return nil
	})
	return out
}

func (b *badgerStore[K, V]) Close() {
	err := b.db.Close()
	if err != nil {
		panic(err)
	}
}
./subscriber/pool.go
package subscriber

import (
	"sync"
)

type SubscriberPool[T any] interface {
	Subscribe(T) func()
	Iterate(func(T))
}

func NewPool[T any]() SubscriberPool[T] {
	return &subscriberPool[T]{
		mu:    sync.RWMutex{},
		count: 0,
		pool:  make(map[uint]T),
	}
}

type subscriberPool[T any] struct {
	mu    sync.RWMutex
	count uint
	pool  map[uint]T
}

func (p *subscriberPool[T]) Subscribe(t T) func() {
	p.mu.Lock()
	defer p.mu.Unlock()
	count := p.count
	p.count++
	p.pool[count] = t
	return func() {
		p.mu.Lock()
		defer p.mu.Unlock()
		delete(p.pool, count)
	}
}

func (p *subscriberPool[T]) Iterate(f func(t T)) {
	p.mu.RLock()
	defer p.mu.RUnlock()
	for _, t := range p.pool {
		f(t)
	}
}
./rpc/dispatcher.go
package rpc

import (
	"encoding/json"
	"fmt"
	"reflect"
)

type Dispatcher interface {
	Register(name string, h any) Dispatcher
	Handle(input []byte) (output []byte, err error)
}

func NewDispatcher() Dispatcher {
	return &dispatcher{
		handlerMap: make(map[string]handler),
	}
}

type message struct {
	Cmd  string `json:"cmd"`
	Body []byte `json:"body"`
}

type handler struct {
	handlerFunc reflect.Value
	argType     reflect.Type
}

type dispatcher struct {
	handlerMap map[string]handler
}

func (d *dispatcher) Register(name string, h any) Dispatcher {
	handlerFunc := reflect.ValueOf(h)
	handlerFuncType := handlerFunc.Type()
	if handlerFuncType.Kind() != reflect.Func || handlerFuncType.NumIn() != 1 || handlerFuncType.NumOut() != 1 {
		panic("handler must be of form func(*SomeRequest) *SomeResponse")
	}
	argType := handlerFuncType.In(0)
	if argType.Kind() != reflect.Ptr || handlerFuncType.Out(0).Kind() != reflect.Ptr {
		panic("handler arguments and return type must be pointers")
	}
	d.handlerMap[name] = handler{
		handlerFunc: handlerFunc,
		argType:     argType,
	}
	return d
}

func (d *dispatcher) Handle(input []byte) (output []byte, err error) {
	msg := message{}
	if err := json.Unmarshal(input, &msg); err != nil {
		return nil, err
	}

	h, ok := d.handlerMap[msg.Cmd]
	if !ok {
		return nil, fmt.Errorf("command not found")
	}

	argPtr := reflect.New(h.argType.Elem()).Interface()
	if err := json.Unmarshal(msg.Body, argPtr); err != nil {
		return nil, err
	}

	out := h.handlerFunc.Call([]reflect.Value{reflect.ValueOf(argPtr)})[0].Interface()

	output, err = json.Marshal(out)
	if err != nil {
		return nil, err
	}
	return output, nil
}
./rpc/tcp_server.go
package rpc

import (
	"bufio"
	"fmt"
	"io"
	"net"
	"sync"
	"time"
)

const (
	TCP_TIMEOUT = 10 * time.Second
)

type TCPServer interface {
	Handle(input []byte) (output []byte, err error)
	ListenAndServe() error
	Register(name string, h any) TCPServer
	Close() error
}

func TCPTransport(addr string) TransportFunc {
	return func(input []byte) (output []byte, err error) {
		conn, err := net.Dial("tcp", addr)
		if err != nil {
			return nil, err
		}
		defer conn.Close()

		err = conn.SetDeadline(time.Now().Add(TCP_TIMEOUT))
		if err != nil {
			return
		}

		conn.Write(input)
		conn.Write([]byte("\n")) // '\n' notifies end of input
		output, err = io.ReadAll(conn)
		return output, err
	}
}

type tcpServer struct {
	mu         sync.Mutex
	dispatcher Dispatcher
	listener   net.Listener
}

func NewTCPServer(bindAddr string) (TCPServer, error) {
	listener, err := net.Listen("tcp", bindAddr)
	if err != nil {
		return nil, err
	}
	return &tcpServer{
		mu:         sync.Mutex{},
		dispatcher: NewDispatcher(),
		listener:   listener,
	}, nil
}

func (s *tcpServer) Close() error {
	return s.listener.Close()
}

func (s *tcpServer) Handle(input []byte) (output []byte, err error) {
	return s.dispatcher.Handle(input)
}

func (s *tcpServer) Register(name string, h any) TCPServer {
	s.mu.Lock()
	defer s.mu.Unlock()
	s.dispatcher.Register(name, h)
	return s
}

func (s *tcpServer) handleConn(conn net.Conn) {
	defer conn.Close()
	err := conn.SetDeadline(time.Now().Add(TCP_TIMEOUT))
	if err != nil {
		return
	}

	msg, err := bufio.NewReader(conn).ReadString('\n') // read until '\n'
	if err != nil {
		return
	}
	b := []byte(msg)
	{
		s.mu.Lock()
		defer s.mu.Unlock()
		b, err = s.dispatcher.Handle(b)
	}
	if err != nil {
		fmt.Println(err)
		return
	}
	_, err = conn.Write(b)
	if err != nil {
		return
	}
}

func (s *tcpServer) ListenAndServe() error {
	for {
		conn, err := s.listener.Accept()
		if err != nil {
			return err
		}
		go s.handleConn(conn)
	}
}
./rpc/rpc.go
package rpc

import (
	"encoding/json"
)

type TransportFunc func([]byte) ([]byte, error)

func zeroPtr[T any]() *T {
	var v T
	return &v
}

func RPC[Req any, Res any](transport TransportFunc, name string, req *Req) (res *Res, err error) {
	body, err := json.Marshal(req)
	if err != nil {
		return nil, err
	}
	msg := message{
		Cmd:  name,
		Body: body,
	}
	b, err := json.Marshal(msg)
	if err != nil {
		return nil, err
	}
	b, err = transport(b)
	if err != nil {
		return nil, err
	}

	res = zeroPtr[Res]()
	if err := json.Unmarshal(b, res); err != nil {
		return nil, err
	}
	return res, nil
}
./main.go
package main

import (
	"encoding/json"
	"fmt"
	"net/http"
	"os"
	"strconv"
	"time"

	"github.com/khanh101/paxos/dist_kvstore"
)

type Config struct {
	Badger string `json:"badger"`
	RPC    string `json:"rpc"`
	Store  string `json:"store"`
}
type ConfigList []Config

func main() {
	b, err := os.ReadFile(os.Args[1])
	if err != nil {
		panic(err)
	}
	var cl ConfigList
	err = json.Unmarshal(b, &cl)
	if err != nil {
		panic(err)
	}

	id, err := strconv.Atoi(os.Args[2])
	if err != nil {
		panic(err)
	}

	badgerDBPath := cl[id].Badger
	peerAddrList := make([]string, len(cl))
	for i, c := range cl {
		peerAddrList[i] = c.RPC
	}
	ds, err := dist_kvstore.NewDistStore(id, badgerDBPath, peerAddrList)
	if err != nil {
		panic(err)
	}
	defer ds.Close()
	go ds.ListenAndServeRPC()
	time.Sleep(time.Second)

	// http server
	hs := &http.Server{
		Addr:    cl[id].Store,
		Handler: dist_kvstore.HttpHandle(ds),
	}
	fmt.Println("http server listening on", cl[id].Store)
	err = hs.ListenAndServe()
	if err != nil {
		panic(err)
	}
	return
}
./client.py
from typing import Iterator, Any
import json
import time
import random

import pydantic
import requests


def make_request(method: str, addr: str, path: str, **kwargs) -> requests.Response:
    res = requests.request(method, f"{addr}/{path}", **kwargs)
    res.raise_for_status()
    return res

class Cmd(pydantic.BaseModel):
    key: str
    val: str
    ver: int

class KVStore:
    def __init__(self, addr: str = "http://localhost:4000"):
        self.addr = addr

    def get(self, key: str) -> Cmd:
        return Cmd.model_validate_json(make_request("GET", self.addr, f"kvstore/{key}").text)

    def set(self, key: str, val: str, ver: int):
        make_request("PUT", self.addr, f"kvstore/{key}", data=json.dumps({"val": val, "ver": ver}))

    def keys(self) -> list[str]:
        return json.loads(make_request("GET", self.addr, "kvstore/").text)

class KVStoreDict:
    def __init__(self, addr: str = "http://localhost:4000"):
        self.kvstore = KVStore(addr)

    def __getitem__(self, key: str) -> Any:
        val = self.kvstore.get(key).val
        if len(val) == 0:
            return None
        return json.loads(val)

    def __setitem__(self, key: str, val: Any):
        if val is None:
            val_s = ""
        else:
            val_s = json.dumps(val)
        wait = 0.001
        while True:
            try:
                self.kvstore.set(key, val_s, self.kvstore.get(key).ver + 1)
                return
            except requests.exceptions.HTTPError as e:
                time.sleep(wait * random.random())
                wait *= 2

    def keys(self) -> list[str]:
        return self.kvstore.keys()

    def __delitem__(self, key: str):
        self.__setitem__(key, None)

    def values(self) -> Iterator[str]:
        for key in self.keys():
            yield self.__getitem__(key)

    def items(self) -> Iterator[tuple[str, str]]:
        for key in self.keys():
            yield key, self.__getitem__(key)

    def __dict__(self) -> dict[str, str]:
        return dict(self.items())

    def __repr__(self) -> str:
        return repr(self.__dict__())
./conf/kvstore.json
[
  {
    "badger": "data/acceptor0",
    "rpc": "localhost:3000",
    "store": "localhost:4000"
  },
  {
    "badger": "data/acceptor1",
    "rpc": "localhost:3001",
    "store": "localhost:4001"
  },
  {
    "badger": "data/acceptor2",
    "rpc": "localhost:3002",
    "store": "localhost:4002"
  }
]./README.md
# kvstore

implementation of distributed kvstore using paxos

paxos is easier to understand and prove unlike raft

[proof](https://github.com/khanh101/khanh101.github.io/blob/master/blog/pdf/paxos-algorithm.pdf)

[pkg.go.dev](https://pkg.go.dev/github.com/khanh101/paxos)

## EXAMPLE

cluster is online if and only if a quorum is online

```bash
go run main.go conf/kvstore.json 0
go run main.go conf/kvstore.json 1
go run main.go conf/kvstore.json 2
```

```bash
# get all keys
curl http://localhost:4000/kvstore/ -X GET
# read, unset key are with '{"val": "", "ver": 0}' by default 
curl http://localhost:4000/kvstore/<key> -X GET
# update key 
curl http://localhost:4000/kvstore/<key> -X PUT -d '{"val": "<value>", "ver": <ver>}'
# delete key
curl http://localhost:4000/kvstore/<key> -X PUT -d '{"val": "", "ver": <ver>}'
```


## TODO 

- implement log compaction (3/5 difficulty, 4/5 complexity)

- implement TLS for RPC and HTTP (1/5)

- implement leader election (3/5)

- implement dynamic membership changes (5/5)

do you have any comment on this?
